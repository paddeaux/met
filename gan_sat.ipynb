{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CRT Placement - Met Éireann"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "from rasterio.plot import show\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from math import log2\n",
    "\n",
    "import random\n",
    "import torchvision\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import Dataset\n",
    "from osgeo import gdal as GD\n",
    "\n",
    "from typing import Tuple, Dict, List\n",
    "import pathlib\n",
    "\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In generator conv blocks, the channels go like \"512->512->512->512->256->128->64->32->16\"\n",
    "factors = [1,1,1,1,1/2,1/4,1/8,1/16,1/32]\n",
    "\n",
    "# Equalized learning rate applied on conv2d layers -- from ProGan paper\n",
    "# in convolution operation: pixel * {weight * (1/norm)} <==> {pixel * (1/norm)} * weight \n",
    "class WSConv2d(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, gain=2\n",
    "    ):\n",
    "        super(WSConv2d, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.scale = (gain / (in_channels * (kernel_size ** 2))) ** 0.5\n",
    "        self.bias = self.conv.bias\n",
    "        self.conv.bias = None\n",
    "\n",
    "        # initialize conv layer\n",
    "        nn.init.normal_(self.conv.weight)\n",
    "        nn.init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x * self.scale) + self.bias.view(1, self.bias.shape[0], 1, 1)\n",
    "\n",
    "class PixelNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PixelNorm, self).__init__()\n",
    "        self.epsilon = 1e-8\n",
    "\n",
    "    # x-shape: Batch Size x Channels x H X W\n",
    "    def forward(self, x):\n",
    "        return x / torch.sqrt(torch.mean(x ** 2, dim=1, keepdim=True) + self.epsilon)\n",
    "\n",
    "# This ConvBlock will be used for both Generator and Discriminator, but inside, pixelnorm(after leakyReLU) is used only in Generator\n",
    "# Be careful: the order is \"conv2d-activation-norm\", not \"conv2d-norm-activation\"\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, use_pixelnorm=True):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.use_pn = use_pixelnorm\n",
    "        self.conv1 = WSConv2d(in_channels, out_channels)\n",
    "        self.conv2 = WSConv2d(out_channels, out_channels)\n",
    "        self.leaky = nn.LeakyReLU(0.2)\n",
    "        self.pn = PixelNorm()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.leaky(self.conv1(x))\n",
    "        x = self.pn(x) if self.use_pn else x\n",
    "        x = self.leaky(self.conv2(x))\n",
    "        x = self.pn(x) if self.use_pn else x\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator & Discriminator (Critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, in_channels, img_channels=3): #channels=13\n",
    "        super(Generator, self).__init__()\n",
    "        # starting structure of Generator should be opposite of ending structure of Discriminator\n",
    "        # initial takes 1x1 -> 4x4\n",
    "        self.initial = nn.Sequential(\n",
    "            PixelNorm(),\n",
    "            nn.ConvTranspose2d(z_dim, in_channels, 4, 1, 0),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            WSConv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            PixelNorm(),\n",
    "        )\n",
    "\n",
    "        self.initial_rgb = WSConv2d(\n",
    "            in_channels, img_channels, kernel_size=1, stride=1, padding=0\n",
    "        )\n",
    "        self.prog_blocks, self.rgb_layers = (\n",
    "            nn.ModuleList([]),\n",
    "            nn.ModuleList([self.initial_rgb]),\n",
    "        )\n",
    "\n",
    "        for i in range(\n",
    "            len(factors) - 1\n",
    "        ):  # -1 to prevent index error because of factors[i+1]\n",
    "            conv_in_c = int(in_channels * factors[i])\n",
    "            conv_out_c = int(in_channels * factors[i + 1])\n",
    "            self.prog_blocks.append(ConvBlock(conv_in_c, conv_out_c))\n",
    "            self.rgb_layers.append(\n",
    "                WSConv2d(conv_out_c, img_channels, kernel_size=1, stride=1, padding=0)\n",
    "            )\n",
    "\n",
    "    def fade_in(self, alpha, upscaled, generated):\n",
    "        # alpha should be scalar within [0, 1], and upscale.shape == generated.shape\n",
    "        return torch.tanh(alpha * generated + (1 - alpha) * upscaled)\n",
    "\n",
    "    def forward(self, x, alpha, steps): ## steps=0 : 4x4 output / steps=1 : 8x8 output / steps=2 : 16x16 output ...\n",
    "        out = self.initial(x)\n",
    "\n",
    "        if steps == 0:\n",
    "            return self.initial_rgb(out)\n",
    "\n",
    "        for step in range(steps):\n",
    "            upscaled = F.interpolate(out, scale_factor=2, mode=\"nearest\")\n",
    "            out = self.prog_blocks[step](upscaled)\n",
    "\n",
    "        # The number of channels in upscale will stay the same, while\n",
    "        # out which has moved through prog_blocks might change. To ensure\n",
    "        # we can convert both to rgb we use different rgb_layers\n",
    "        # (steps-1) and steps for upscaled, out respectively\n",
    "        final_upscaled = self.rgb_layers[steps - 1](upscaled)\n",
    "        final_out = self.rgb_layers[steps](out)\n",
    "        return self.fade_in(alpha, final_upscaled, final_out)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels, img_channels=3): #channels=13\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.prog_blocks, self.rgb_layers = nn.ModuleList([]), nn.ModuleList([])\n",
    "        self.leaky = nn.LeakyReLU(0.2)\n",
    "\n",
    "        # here we work back ways from factors because the discriminator\n",
    "        # should be mirrored from the generator. So the first prog_block and\n",
    "        # rgb layer we append will work for input size 1024x1024, then 512->256-> etc\n",
    "        for i in range(len(factors) - 1, 0, -1):\n",
    "            conv_in = int(in_channels * factors[i])\n",
    "            conv_out = int(in_channels * factors[i - 1])\n",
    "            \n",
    "            ## rgb_layers list : [(1024x1024 input'from_rgb' layer),(512x512 input'from_rgb' layer),(256x256 input'from_rgb' layer) ...]\n",
    "            self.prog_blocks.append(ConvBlock(conv_in, conv_out, use_pixelnorm=False))\n",
    "            self.rgb_layers.append(\n",
    "                WSConv2d(img_channels, conv_in, kernel_size=1, stride=1, padding=0)\n",
    "            )\n",
    "\n",
    "        ## this 'from-rgb' layer is for 4x4 resolution\n",
    "        # did this to \"mirror\" the generator initial_rgb\n",
    "        self.initial_rgb = WSConv2d(\n",
    "            img_channels, in_channels, kernel_size=1, stride=1, padding=0\n",
    "        )\n",
    "        self.rgb_layers.append(self.initial_rgb)\n",
    "        self.avg_pool = nn.AvgPool2d(\n",
    "            kernel_size=2, stride=2\n",
    "        )  # down sampling using avg pool\n",
    "\n",
    "        ## ConvBlock for 4x4 resolution\n",
    "        self.final_block = nn.Sequential(\n",
    "            # +1 to in_channels because we concatenate from MiniBatch std\n",
    "            WSConv2d(in_channels + 1, in_channels, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            WSConv2d(in_channels, in_channels, kernel_size=4, padding=0, stride=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            WSConv2d(\n",
    "                in_channels, 1, kernel_size=1, padding=0, stride=1\n",
    "            ),  # we use this instead of linear layer\n",
    "        )\n",
    "\n",
    "    def fade_in(self, alpha, downscaled, out):\n",
    "        \"\"\"Used to fade in downscaled using avg pooling and output from CNN\"\"\"\n",
    "        # alpha should be scalar within [0, 1], and upscale.shape == generated.shape\n",
    "        return alpha * out + (1 - alpha) * downscaled\n",
    "\n",
    "    def minibatch_std(self, x):     ## NxCxHxW -> N -> 1 -> Nx1xHxW\n",
    "        batch_statistics = (\n",
    "            torch.std(x, dim=0).mean().repeat(x.shape[0], 1, x.shape[2], x.shape[3])\n",
    "        )\n",
    "        # we take the std for each example (across all channels, and pixels) then we repeat it\n",
    "        # for a single channel and concatenate it with the image. In this way the discriminator\n",
    "        # will get information about the variation in the batch/image\n",
    "        return torch.cat([x, batch_statistics], dim=1)\n",
    "\n",
    "    def forward(self, x, alpha, steps):     ## steps=0 : 4x4 input / steps=1 : 8x8 input ...\n",
    "        # where we should start in the list of prog_blocks, maybe a bit confusing but\n",
    "        # the last is for the 4x4. So example let's say steps=1, then we should start\n",
    "        # at the second to last because input_size will be 8x8. If steps==0 we just\n",
    "        # use the final block\n",
    "        cur_step = len(self.prog_blocks) - steps\n",
    "\n",
    "        # convert from rgb as initial step, this will depend on\n",
    "        # the image size (each will have it's on rgb layer)\n",
    "        out = self.leaky(self.rgb_layers[cur_step](x))\n",
    "\n",
    "        if steps == 0:  # i.e, image is 4x4\n",
    "            out = self.minibatch_std(out)\n",
    "            return self.final_block(out).view(out.shape[0], -1)\n",
    "\n",
    "        # because prog_blocks might change the channels, for down scale we use rgb_layer\n",
    "        # from previous/smaller size which in our case correlates to +1 in the indexing\n",
    "        downscaled = self.leaky(self.rgb_layers[cur_step + 1](self.avg_pool(x)))\n",
    "        out = self.avg_pool(self.prog_blocks[cur_step](out))\n",
    "\n",
    "        # the fade_in is done first between the downscaled and the input\n",
    "        # this is opposite from the generator\n",
    "        out = self.fade_in(alpha, downscaled, out)\n",
    "\n",
    "        for step in range(cur_step + 1, len(self.prog_blocks)):\n",
    "            out = self.prog_blocks[step](out)\n",
    "            out = self.avg_pool(out)\n",
    "\n",
    "        out = self.minibatch_std(out)\n",
    "        return self.final_block(out).view(out.shape[0], -1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_on_tensorboard(writer,loss_critic,loss_gen,real,fake,tensorboard_step):\n",
    "    writer.add_scalar(\"Loss Critic\",loss_critic,global_step=tensorboard_step)\n",
    "    writer.add_scalar(\"Loss Generator\", loss_gen, global_step=tensorboard_step)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        img_grid_real = torchvision.utils.make_grid(real[:8],normalize=True)\n",
    "        img_grid_fake = torchvision.utils.make_grid(fake[:8],normalize=True)\n",
    "        \n",
    "        writer.add_image(\"Real\",img_grid_real,global_step = tensorboard_step)\n",
    "        writer.add_image(\"Fake\",img_grid_fake,global_step = tensorboard_step)\n",
    "        \n",
    "def gradient_penalty(critic,real,fake,alpha,train_step,device=\"cpu\"):    \n",
    "    BATCH_SIZE,C,H,W = real.shape\n",
    "    beta = torch.rand((BATCH_SIZE,1,1,1)).repeat(1,C,H,W).to(device)\n",
    "\n",
    "    interpolated_images = real * beta + fake.detach() * (1-beta)\n",
    "    interpolated_images.requires_grad_(True)\n",
    "    \n",
    "    ## Calculate critic scores\n",
    "    mixed_scores = critic(interpolated_images,alpha,train_step)\n",
    "    \n",
    "    ## Take the gradient of the scores with respect to the image\n",
    "    gradient = torch.autograd.grad(\n",
    "        inputs = interpolated_images,\n",
    "        outputs = mixed_scores,\n",
    "        grad_outputs = torch.ones_like(mixed_scores),\n",
    "        create_graph = True,\n",
    "        retain_graph = True\n",
    "    )[0]\n",
    "    \n",
    "    gradient = gradient.view(gradient.shape[0],-1)\n",
    "    gradient_norm = gradient.norm(2,dim=1)\n",
    "    penalty = torch.mean((gradient_norm - 1)**2)\n",
    "    return penalty\n",
    "\n",
    "def save_checkpoint(model,optimizer,filename=\"my_checkpoint.pth\"):\n",
    "    print(\"Saving Checkpoint\")\n",
    "    checkpoint = {\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"optimizer\" : optimizer.state_dict()\n",
    "    }\n",
    "    torch.save(checkpoint,filename)\n",
    "    \n",
    "def load_checkpoint(checkpoint_file,model,optimizer,lr):\n",
    "    print(\"Loading Checkpoint\")\n",
    "    checkpoint = torch.load(checkpoint_file,map_location=\"cuda\")\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "    \n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr\n",
    "        \n",
    "def generate_examples(gen,current_epoch,steps,n=16):\n",
    "    gen.eval()\n",
    "    alpha = 1.0\n",
    "    \n",
    "    for i in range(n):\n",
    "        with torch.no_grad():\n",
    "            noise = torch.randn(1,Z_DIM,1,1).to(DEVICE)\n",
    "            generated_img = gen(noise,alpha=alpha,steps=steps)\n",
    "            save_image(generated_img*0.5+0.5, f\"generated_images/step{steps}_epoch{current_epoch}_{i}.png\")\n",
    "            torch.save(generated_img*0.5+0.5, f\"generated_images/step{steps}_epoch{current_epoch}_{i}.pt\")\n",
    "    gen.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEN12MS Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Make function to find classes in target directory\n",
    "def find_classes(directory: str) -> Tuple[List[str], Dict[str, int]]:\n",
    "    \"\"\"Finds the class folder names in a target directory.\n",
    "    \n",
    "    Assumes target directory is in standard image classification format.\n",
    "\n",
    "    Args:\n",
    "        directory (str): target directory to load classnames from.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[str], Dict[str, int]]: (list_of_class_names, dict(class_name: idx...))\n",
    "    \n",
    "    Example:\n",
    "        find_classes(\"food_images/train\")\n",
    "        >>> ([\"class_1\", \"class_2\"], {\"class_1\": 0, ...})\n",
    "    \"\"\"\n",
    "    # 1. Get the class names by scanning the target directory\n",
    "    classes = sorted(entry.name for entry in os.scandir(directory) if entry.is_dir())\n",
    "    \n",
    "    # 2. Raise an error if class names not found\n",
    "    if not classes:\n",
    "        raise FileNotFoundError(f\"Couldn't find any classes in {directory}.\")\n",
    "        \n",
    "    # 3. Crearte a dictionary of index labels (computers prefer numerical rather than string labels)\n",
    "    class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n",
    "    return classes, class_to_idx\n",
    "\n",
    "class SEN12MS(Dataset):\n",
    "    \"\"\"Custom dataset for SEN12MS data\"\"\"\n",
    "\n",
    "    # 2. Initialize with a targ_dir and transform (optional) parameter\n",
    "    def __init__(self, targ_dir: str, transform=None) -> None:\n",
    "        \n",
    "        # 3. Create class attributes\n",
    "        # Get all image paths\n",
    "        self.paths = list(pathlib.Path(targ_dir).glob(\"*/*.tif\")) # note: you'd have to update this if you've got .png's or .jpeg's\n",
    "        # Setup transforms\n",
    "        self.transform = transform\n",
    "        # Create classes and class_to_idx attributes\n",
    "        self.classes, self.class_to_idx = find_classes(targ_dir)\n",
    "\n",
    "    # 4. Make function to load images\n",
    "    def load_image(self, index: int) -> Image.Image:\n",
    "        \"Opens an image via a path and returns it.\"\n",
    "        image_path = self.paths[index]\n",
    "        image = rasterio.open(str(image_path)).read([2,3,4])\n",
    "        image_norm = cv2.normalize(image.astype(np.float32), dst=None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX)\n",
    "        #norm_image = (image - image.min()) / (image.max() - image.min())\n",
    "        return np.transpose(image_norm,(1,2,0))\n",
    "    \n",
    "    # 5. Overwrite the __len__() method (optional but recommended for subclasses of torch.utils.data.Dataset)\n",
    "    def __len__(self) -> int:\n",
    "        \"Returns the total number of samples.\"\n",
    "        return len(self.paths)\n",
    "    \n",
    "    # 6. Overwrite the __getitem__() method (required for subclasses of torch.utils.data.Dataset)\n",
    "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, int]:\n",
    "        \"Returns one sample of data, data and label (X, y).\"\n",
    "        img = self.load_image(index)\n",
    "        class_name  = self.paths[index].parent.name # expects path in data_folder/class_name/image.jpeg\n",
    "        class_idx = self.class_to_idx[class_name]\n",
    "\n",
    "        # Transform if necessary\n",
    "        if self.transform:\n",
    "            return self.transform(img), class_idx # return data, label (X, y)\n",
    "        else:\n",
    "            return img, class_idx # return data, label (X, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(img_size):\n",
    "    transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((img_size,img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.Normalize([0.5 for _ in range(IMG_CHANNELS)],[0.5 for _ in range(IMG_CHANNELS)])\n",
    "    ])\n",
    "\n",
    "    transform_sen = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((img_size,img_size)),\n",
    "        #transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.Normalize([0.5 for _ in range(IMG_CHANNELS)],[0.5 for _ in range(IMG_CHANNELS)])\n",
    "    ])\n",
    "    \n",
    "    batch_size = BATCH_SIZES[int(log2(img_size/4))]\n",
    "    #dataset = datasets.ImageFolder(root=DATASET,transform=transform)\n",
    "    dataset = SEN12MS(targ_dir=DATASET, transform=transform_sen)\n",
    "\n",
    "    print(\"input shape:\", dataset[0][0].shape)\n",
    "\n",
    "    loader = DataLoader(dataset,batch_size=batch_size,shuffle=True,num_workers=NUM_WORKERS,pin_memory=True) #batch_size\n",
    "    \n",
    "    return loader,dataset\n",
    "\n",
    "def train_fn(gen,critic,loader,dataset,step,alpha,opt_gen,opt_critic,tensorboard_step,writer,scaler_gen,scaler_critic):\n",
    "    loop = tqdm(loader,leave=True)\n",
    "    i = 0\n",
    "    \n",
    "    for batch_idx, (real, _) in enumerate(loop):#batch_idx,(real,_) in enumerate(loop):\n",
    "        i += 1\n",
    "        if i%2 == 0:\n",
    "            continue\n",
    "        real = real.to(DEVICE)\n",
    "        cur_batch_size = real.shape[0]\n",
    "        noise = torch.randn(cur_batch_size,Z_DIM,1,1).to(DEVICE)\n",
    "        \n",
    "        ## Train Critic\n",
    "        ## Wasserstein Loss : Maximize \"E[Critic(real)] - E[Critic(fake)]\"   ==   Minimize \"-(E[Critic(real)] - E[Critic(fake)])\"\n",
    "        with torch.cuda.amp.autocast():\n",
    "            fake = gen(noise,alpha,step).to(DEVICE)\n",
    "            critic_real = critic(real,alpha,step)\n",
    "            critic_fake = critic(fake.detach(),alpha,step)\n",
    "            gp = gradient_penalty(critic,real,fake,alpha,step,device=DEVICE)\n",
    "            loss_critic = -1 * (torch.mean(critic_real) - torch.mean(critic_fake)) + LAMBDA_GP * gp + 0.001 * torch.mean(critic_real**2)\n",
    "        \n",
    "        critic.zero_grad()\n",
    "        scaler_critic.scale(loss_critic).backward()\n",
    "        scaler_critic.step(opt_critic)\n",
    "        scaler_critic.update()\n",
    "        \n",
    "        ## Train Generator\n",
    "        ## Maximize \"E[Critic(fake)]\"   ==   Minimize \"- E[Critic(fake)]\"\n",
    "        with torch.cuda.amp.autocast():\n",
    "            gen_fake = critic(fake,alpha,step)\n",
    "            loss_gen = -1 * torch.mean(gen_fake)\n",
    "            \n",
    "        gen.zero_grad()\n",
    "        scaler_gen.scale(loss_gen).backward()\n",
    "        scaler_gen.step(opt_gen)\n",
    "        scaler_gen.update()\n",
    "    \n",
    "        alpha += (cur_batch_size/len(dataset)) * (1/PROGRESSIVE_EPOCHS[step]) * 2\n",
    "        alpha = min(alpha,1)\n",
    "        \n",
    "        if batch_idx % 500 == 0:\n",
    "            with torch.no_grad():\n",
    "                fixed_fakes = gen(FIXED_NOISE,alpha,step) * 0.5 + 0.5\n",
    "                #save_on_tensorboard(writer,loss_critic.item(),loss_gen.item(),real.detach(),fixed_fakes.detach(),tensorboard_step)\n",
    "                tensorboard_step += 1\n",
    "    \n",
    "    return tensorboard_step,alpha"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config & Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Paddy\\CRT\\Github\n",
      "cuda\n",
      "c:\\Users\\Paddy\\CRT\\Github\\input/ROIs1158_spring/test/\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "START_TRAIN_IMG_SIZE = 16\n",
    "DATASET = os.path.join(os.path.dirname(os.getcwd()), \"input/ROIs1158_spring/test/\")\n",
    "print(os.path.dirname(os.getcwd()))\n",
    "\n",
    "CHECKPOINT_GEN = \"Generator_Attempt_RGB.pt\"\n",
    "CHECKPOINT_CRITIC = \"Critic_Attempt_RGB.pt\"\n",
    "SAVE_MODEL = True\n",
    "LOAD_MODEL = False\n",
    "\n",
    "LR = 1e-3\n",
    "BATCH_SIZES = [256,256,128,64,32,16,8] ## modifiable/ Batch_sizes for each step\n",
    "IMAGE_SIZE = 256 ## 1024 for paper\n",
    "IMG_CHANNELS = 3 #13\n",
    "Z_DIM = 256 ## 512 for paper\n",
    "IN_CHANNELS = 256 ## 512 for paper\n",
    "LAMBDA_GP = 10\n",
    "NUM_STEPS = int(log2(IMAGE_SIZE/4)) + 1\n",
    "\n",
    "PROGRESSIVE_EPOCHS = [4] * len(BATCH_SIZES)\n",
    "FIXED_NOISE = torch.randn(8,Z_DIM,1,1).to(DEVICE)\n",
    "# NUM_WORKERS = 4\n",
    "NUM_WORKERS = 0 # have to set to zero otherwise get an error\n",
    "torch.backends.cudnn.benchmarks = True\n",
    "\n",
    "\n",
    "print(DEVICE)\n",
    "print(DATASET)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Paddy\\miniconda3\\envs\\met\\Lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([3, 16, 16])\n",
      "Image size:16 | Current step:2\n",
      "Epoch [1/4] Global Epoch:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 66/66 [08:03<00:00,  7.33s/it]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory generated_images does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 54\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTraining finished\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     52\u001b[0m     \u001b[39mreturn\u001b[39;00m gen, critic\n\u001b[1;32m---> 54\u001b[0m gen, critic \u001b[39m=\u001b[39m train_progan()\n",
      "Cell \u001b[1;32mIn[8], line 39\u001b[0m, in \u001b[0;36mtrain_progan\u001b[1;34m()\u001b[0m\n\u001b[0;32m     37\u001b[0m global_epoch \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     38\u001b[0m \u001b[39mif\u001b[39;00m global_epoch \u001b[39min\u001b[39;00m generate_examples_at:\n\u001b[1;32m---> 39\u001b[0m     generate_examples(gen,global_epoch,step,n\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m)\n\u001b[0;32m     41\u001b[0m \u001b[39mif\u001b[39;00m SAVE_MODEL \u001b[39mand\u001b[39;00m (epoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m%\u001b[39m\u001b[39m8\u001b[39m\u001b[39m==\u001b[39m\u001b[39m0\u001b[39m:\n\u001b[0;32m     42\u001b[0m     save_checkpoint(gen,opt_gen,filename\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCHECKPOINT_GEN.pt\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 62\u001b[0m, in \u001b[0;36mgenerate_examples\u001b[1;34m(gen, current_epoch, steps, n)\u001b[0m\n\u001b[0;32m     60\u001b[0m         generated_img \u001b[39m=\u001b[39m gen(noise,alpha\u001b[39m=\u001b[39malpha,steps\u001b[39m=\u001b[39msteps)\n\u001b[0;32m     61\u001b[0m         \u001b[39m#save_image(generated_img*0.5+0.5, f\"generated_images/step{steps}_epoch{current_epoch}_{i}.png\")\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m         torch\u001b[39m.\u001b[39;49msave(generated_img\u001b[39m*\u001b[39;49m\u001b[39m0.5\u001b[39;49m\u001b[39m+\u001b[39;49m\u001b[39m0.5\u001b[39;49m, \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mgenerated_images/step\u001b[39;49m\u001b[39m{\u001b[39;49;00msteps\u001b[39m}\u001b[39;49;00m\u001b[39m_epoch\u001b[39;49m\u001b[39m{\u001b[39;49;00mcurrent_epoch\u001b[39m}\u001b[39;49;00m\u001b[39m_\u001b[39;49m\u001b[39m{\u001b[39;49;00mi\u001b[39m}\u001b[39;49;00m\u001b[39m.pt\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     63\u001b[0m gen\u001b[39m.\u001b[39mtrain()\n",
      "File \u001b[1;32mc:\\Users\\Paddy\\miniconda3\\envs\\met\\Lib\\site-packages\\torch\\serialization.py:440\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[0;32m    437\u001b[0m _check_save_filelike(f)\n\u001b[0;32m    439\u001b[0m \u001b[39mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m--> 440\u001b[0m     \u001b[39mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[39mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m    441\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001b[0;32m    442\u001b[0m         \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Paddy\\miniconda3\\envs\\met\\Lib\\site-packages\\torch\\serialization.py:315\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[1;34m(name_or_buffer)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    314\u001b[0m     container \u001b[39m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[1;32m--> 315\u001b[0m \u001b[39mreturn\u001b[39;00m container(name_or_buffer)\n",
      "File \u001b[1;32mc:\\Users\\Paddy\\miniconda3\\envs\\met\\Lib\\site-packages\\torch\\serialization.py:288\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 288\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49mPyTorchFileWriter(\u001b[39mstr\u001b[39;49m(name)))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Parent directory generated_images does not exist."
     ]
    }
   ],
   "source": [
    "def train_progan():\n",
    "    ## build model\n",
    "    gen = Generator(Z_DIM,IN_CHANNELS,IMG_CHANNELS).to(DEVICE)\n",
    "    critic = Discriminator(IN_CHANNELS,IMG_CHANNELS).to(DEVICE)\n",
    "\n",
    "    ## initialize optimizer,scalers (for FP16 training)\n",
    "    opt_gen = optim.Adam(gen.parameters(),lr=LR,betas=(0.0,0.99))\n",
    "    opt_critic = optim.Adam(critic.parameters(),lr=LR,betas=(0.0,0.99))\n",
    "    scaler_gen = torch.cuda.amp.GradScaler()\n",
    "    scaler_critic = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    ## tensorboard writer\n",
    "    writer = SummaryWriter(f\"runs/PG_GAN\")\n",
    "    tensorboard_step = 0\n",
    "\n",
    "    ## if checkpoint files exist, load model\n",
    "    if LOAD_MODEL:\n",
    "        load_checkpoint(CHECKPOINT_GEN,gen,opt_gen,LR)\n",
    "        load_checkpoint(CHECKPOINT_CRITIC,critic,opt_critic,LR)\n",
    "        \n",
    "    gen.train()\n",
    "    critic.train()\n",
    "\n",
    "    step = int(log2(START_TRAIN_IMG_SIZE/4)) ## starts from 0\n",
    "\n",
    "    global_epoch = 0\n",
    "    generate_examples_at = [1,4,8,12,16,20,24,28,32]\n",
    "\n",
    "    for num_epochs in PROGRESSIVE_EPOCHS[step:]:\n",
    "        alpha = 1e-4\n",
    "        loader,dataset = get_loader(4*2**step)\n",
    "        print(f\"Image size:{4*2**step} | Current step:{step}\")\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}] Global Epoch:{global_epoch}\")\n",
    "            tensorboard_step,alpha = train_fn(gen,critic,loader,dataset,step,alpha,opt_gen,opt_critic,tensorboard_step,writer,scaler_gen,scaler_critic)\n",
    "            global_epoch += 1\n",
    "            if global_epoch in generate_examples_at:\n",
    "                generate_examples(gen,global_epoch,step,n=3)\n",
    "            \n",
    "            if SAVE_MODEL and (epoch+1)%8==0:\n",
    "                save_checkpoint(gen,opt_gen,filename=\"CHECKPOINT_GEN.pt\")\n",
    "                save_checkpoint(critic,opt_critic,filename=\"CHECKPOINT_CRITIC.pt\")\n",
    "                \n",
    "        step += 1 ## Progressive Growing\n",
    "\n",
    "    if SAVE_MODEL:\n",
    "        save_checkpoint(gen,opt_gen,filename=\"CHECKPOINT_GEN.pt\")\n",
    "        save_checkpoint(critic,opt_critic,filename=\"CHECKPOINT_CRITIC.pt\")\n",
    "\n",
    "    print(\"Training finished\")\n",
    "    return gen, critic\n",
    "\n",
    "gen, critic = train_progan()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_progan():\n",
    "    ## build model\n",
    "    gen = Generator(Z_DIM,IN_CHANNELS,IMG_CHANNELS).to(DEVICE)\n",
    "    critic = Discriminator(IN_CHANNELS,IMG_CHANNELS).to(DEVICE)\n",
    "\n",
    "    ## initialize optimizer,scalers (for FP16 training)\n",
    "    opt_gen = optim.Adam(gen.parameters(),lr=LR,betas=(0.0,0.99))\n",
    "    opt_critic = optim.Adam(critic.parameters(),lr=LR,betas=(0.0,0.99))\n",
    "    scaler_gen = torch.cuda.amp.GradScaler()\n",
    "    scaler_critic = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    ## tensorboard writer\n",
    "    writer = SummaryWriter(f\"runs/PG_GAN\")\n",
    "    tensorboard_step = 0\n",
    "\n",
    "    ## if checkpoint files exist, load model\n",
    "    if LOAD_MODEL:\n",
    "        load_checkpoint(CHECKPOINT_GEN,gen,opt_gen,LR)\n",
    "        load_checkpoint(CHECKPOINT_CRITIC,critic,opt_critic,LR)\n",
    "        \n",
    "    gen.train()\n",
    "    critic.train()\n",
    "\n",
    "    print(\"Models loaded\")\n",
    "    return gen, critic\n",
    "\n",
    "#gen, critic = load_progan()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sample(gen,steps=6,n=16):\n",
    "    gen.eval()\n",
    "    with torch.no_grad():\n",
    "        noise = torch.randn(1,Z_DIM,1,1).to(DEVICE)\n",
    "        generated_img = gen(noise,alpha=1,steps=steps)\n",
    "        img = generated_img.detach().cpu().numpy()\n",
    "        #for i in range(img.shape[1]):\n",
    "            #band = img[0][i]\n",
    "        R = img[0][1]\n",
    "        G = img[0][2]\n",
    "        B = img[0][3]\n",
    "        #img_out = np.dstack(band)\n",
    "        img_out = np.dstack((R,G,B))\n",
    "\n",
    "        print(img_out.shape)\n",
    "        show(np.transpose(img_out,(2,1,0)))\n",
    "    gen.train()\n",
    "\n",
    "def plot_sample_bands(gen,steps=6,n=16):\n",
    "    gen.eval()\n",
    "    with torch.no_grad():\n",
    "        noise = torch.randn(1,Z_DIM,1,1).to(DEVICE)\n",
    "        generated_img = gen(noise,alpha=0.5,steps=steps)\n",
    "        print((generated_img*0.5+0.5)[0].shape)\n",
    "        img = generated_img.detach().cpu().numpy()\n",
    "        print(img.shape)\n",
    "        for i in range(img.shape[1]):\n",
    "            band = img[0][i]\n",
    "            img_out = np.dstack(band)\n",
    "            print(img_out.shape)\n",
    "            show(np.transpose(img_out,(2,1,0)))\n",
    "    gen.train()\n",
    "\n",
    "plot_sample(gen)\n",
    "#plot_sample_bands(gen)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "met",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
